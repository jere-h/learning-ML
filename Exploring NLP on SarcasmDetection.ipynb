{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":159,"outputs":[{"output_type":"stream","text":"/kaggle/input/deepnlp/Sheet_2.csv\n/kaggle/input/deepnlp/Sheet_1.csv\n/kaggle/input/glove840b300dtxt/glove.840B.300d.txt\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score\nstop_words = stopwords.words('english')","execution_count":160,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/deepnlp/Sheet_1.csv')\ncolnames = ['response_text', 'response_id', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7']\n\n# Collapse response columns into response_text\ndf['response_text'] = df['response_text'].astype('str')\ndf[colnames[0]] += df[colnames[2:7]].fillna('').sum(1)\ndf = df.drop(columns=colnames[1:7])\n\ndf['class'].value_counts()","execution_count":205,"outputs":[{"output_type":"execute_result","execution_count":205,"data":{"text/plain":"not_flagged    55\nflagged        25\nName: class, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label encoding\ndf['class'] = preprocessing.LabelEncoder().fit_transform(df['class'])\n\nxtrain, xvalid, ytrain, yvalid = \\\n        train_test_split(df['response_text'], df['class'], random_state=1)\n\n# df['class'].value_counts()  # flagged is '0'\n\n# Plot some distribution\ndf.loc[df['class'] == 1, 'response_text'].str.len().apply(np.log1p).hist(label='no flag', alpha=.5)\ndf.loc[df['class'] == 0, 'response_text'].str.len().apply(np.log1p).hist(label='flagged', alpha=.5)\nplt.legend(); # we can see flagged responses tend to be longer","execution_count":206,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Using pre trained word vectors eg GloVe, word2vec, fasttext\n\n# load the GloVe vectors in a dictionary:\nembeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt', encoding='utf8')\nfor line in tqdm(f):\n    values = line.split()\n    word = ''.join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))\n\n# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","execution_count":203,"outputs":[{"output_type":"stream","text":"2196018it [05:40, 6456.26it/s]\n  3%|▎         | 2/60 [00:00<00:00, 1977.05it/s]\n 10%|█         | 2/20 [00:00<00:00, 2658.83it/s]","name":"stderr"},{"output_type":"stream","text":"Found 2195893 word vectors.\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create sentence vectors using the above function for training and validation set\n# tqdm is a loading bar lol\nfrom tqdm import tqdm\nxtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\nxvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]\n\n# convert sentence vectors to array\nxtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)\n\n# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n\n#Fit and predict using XGBoost\nclf.fit(xtrain_glove, ytrain)\npreds_train = clf.predict(xtrain_glove)\npreds_valid = clf.predict(xvalid_glove)\n\nprint('Train acc:', accuracy_score(ytrain, preds_train)) # 1.0\nprint('Val acc:', accuracy_score(yvalid, preds_valid)) # 0.95","execution_count":213,"outputs":[{"output_type":"stream","text":"100%|██████████| 60/60 [00:00<00:00, 1173.68it/s]\n100%|██████████| 20/20 [00:00<00:00, 1129.78it/s]\n","name":"stderr"},{"output_type":"stream","text":"[16:11:34] WARNING: /workspace/src/learner.cc:480: \nParameters: { silent } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\nTrain acc: 1.0\nVal acc: 0.95\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Deep Learning \n# scale the data before any neural net:\nscl = preprocessing.StandardScaler()\nxtrain_glove_scl = scl.fit_transform(xtrain_glove)\nxvalid_glove_scl = scl.transform(xvalid_glove)\n\n# we need to binarize the labels -- run to_categorical twice \nytrain_enc = np_utils.to_categorical(ytrain)\nyvalid_enc = np_utils.to_categorical(yvalid)\n\n# Create 3 layer sequential neural net for starters\nmodel = Sequential()\n\nmodel.add(Dense(3000, input_dim=300, activation='relu')) #300 because used 300d GloVe\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(1600, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(2))              # Set to num of classes\nmodel.add(Activation('softmax'))\n\n# compile the model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# fit the model\nmodel.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n          epochs=5, verbose=1, \n          validation_data=(xvalid_glove_scl, yvalid_enc))\n# train loss: 0.01, val loss: 0.19 ","execution_count":257,"outputs":[{"output_type":"stream","text":"Train on 60 samples, validate on 20 samples\nEpoch 1/5\n60/60 [==============================] - 1s 9ms/step - loss: 1.1948 - val_loss: 0.3641\nEpoch 2/5\n60/60 [==============================] - 0s 1ms/step - loss: 0.5618 - val_loss: 0.2599\nEpoch 3/5\n60/60 [==============================] - 0s 1ms/step - loss: 0.0431 - val_loss: 0.2202\nEpoch 4/5\n60/60 [==============================] - 0s 981us/step - loss: 0.0379 - val_loss: 0.2004\nEpoch 5/5\n60/60 [==============================] - 0s 993us/step - loss: 0.0134 - val_loss: 0.1932\n","name":"stdout"},{"output_type":"execute_result","execution_count":257,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f23beae6690>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Try LSTM model, but tokenize the text data first\n# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 70\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n# zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index\n\n# create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","execution_count":258,"outputs":[{"output_type":"stream","text":"100%|██████████| 678/678 [00:00<00:00, 106570.91it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Simple LSTM with glove embeddings and two dense layers\nmodel = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=max_len,\n                     trainable=False))\n\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(50, dropout=0.6, recurrent_dropout=0.2))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(2))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n\nmodel.fit(xtrain_pad, y=ytrain_enc, epochs=20, \n          verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])\n\n# Val loss 0.55 ... how to adjust hyperparameters? Hmm... read up more","execution_count":276,"outputs":[{"output_type":"stream","text":"Train on 60 samples, validate on 20 samples\nEpoch 1/20\n60/60 [==============================] - 1s 11ms/step - loss: 0.6852 - val_loss: 0.6157\nEpoch 2/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.6451 - val_loss: 0.5776\nEpoch 3/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.6142 - val_loss: 0.5645\nEpoch 4/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.6240 - val_loss: 0.5574\nEpoch 5/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.5493\nEpoch 6/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.5734 - val_loss: 0.5425\nEpoch 7/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.5488 - val_loss: 0.5409\nEpoch 8/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.5034 - val_loss: 0.5354\nEpoch 9/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.4475 - val_loss: 0.5311\nEpoch 10/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.4334 - val_loss: 0.5231\nEpoch 11/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.3673 - val_loss: 0.5211\nEpoch 12/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.3566 - val_loss: 0.5256\nEpoch 13/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.2994 - val_loss: 0.5405\nEpoch 14/20\n60/60 [==============================] - 0s 2ms/step - loss: 0.3350 - val_loss: 0.5738\n","name":"stdout"},{"output_type":"execute_result","execution_count":276,"data":{"text/plain":"<keras.callbacks.callbacks.History at 0x7f23ad8e59d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}