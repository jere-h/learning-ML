{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!ls ../input/sarcasm/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After importing and doing basic data exploration using \"train_df.info()\", we notice a few NULL entries in comments that can be dropped.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')\ntrain_df.dropna(subset=['comment'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split data (huge dataset, 1m rows) into train and val sets.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts, valid_texts, ytrain, yvalid = \\\n        train_test_split(train_df['comment'], train_df['label'], train_size = 0.6, random_state=17)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start with **data exploration** by looking at things like:\n<br> - Length distribution of comments for each class\n<br> - Word cloud of most common words for each class (not very useful)\n<br> - Group by other categories (Subreddit, author,..) to get rankings and details sorted by 'sum', 'mean', etc.\n<br> To get an idea whether other features will be helpful in classification, not just the comments.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Distribution of data for each class\ntrain_df.loc[train_df['label'] == 1, 'comment'].str.len().apply(np.log1p).hist(label='sarcastic', alpha=.5)\ntrain_df.loc[train_df['label'] == 0, 'comment'].str.len().apply(np.log1p).hist(label='normal', alpha=.5)\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# View stats/details when grouped by other categories like subreddit\nsub_df = train_df.groupby('subreddit')['label'].agg([np.size, np.mean, np.sum])\nsub_df.sort_values(by='sum', ascending=False).head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Creating a log loss metric function. Still not sure how this works or whether there are packages for it now.\n<br>Can also use Accuracy as a metric.\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a multi-class log loss metric function\ndef multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 / rows * vsota","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Starting from basic models as a sanity check.\n<br>First vectorize words in comments from train/test data (ie. turning them to numbers).\n<br>After which they are passed to a simple LR model and evaluated on logloss.\n<br><br>*Term frequencyâ€“inverse document frequency measures 'importance',<br> converts words into numbers that account for rarity of a word and its frequency. * ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Trying out Tf-id vectorize -> Logistic Regression model\n   \n# Always start with these features. They work (almost) everytime!    Need to check how the preprocessing of stopwords/punctuation/stemming is done?\n#tfv = TfidfVectorizer(min_df=3,  max_features=None, \n            #strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n           # ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n           # stop_words = 'english')\ntfv = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)  # from the 'suggested answer'\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(train_texts) + list(valid_texts))\nxtrain_tfv =  tfv.transform(train_texts) \nxvalid_tfv = tfv.transform(valid_texts)\n\n# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1, n_jobs=4, solver='lbfgs', random_state=17, verbose=1) #from the 'suggested answer'\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"CountVectorizer is like tf-idf but without the idf portion that accounts for rarity.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(train_texts) + list(valid_texts))\nxtrain_ctv =  ctv.transform(train_texts) \nxvalid_ctv = ctv.transform(valid_texts)\n\n# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now try Naive Bayes, which is a probabilistic model that accounts for probability of terms but not their order.\n<br> A simplistic approach that takes the product of multiple likelihoods, eg. p(Class 1) x p(Word 1 | Class 1) x p(Word 2 | Class 2) ...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple Naive Bayes on TFIDF\nclf = MultinomialNB()\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now use Singular Value Decomposition (dimensionality reduction, too many features), \n<br>then use standard scaler and then pass it to a Support Vector Classifier model to train.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply SVD, I chose 120 components. 120-200 components are good enough for SVM model.\nsvd = decomposition.TruncatedSVD(n_components=120)\nsvd.fit(xtrain_tfv)                                     #SVD and Scale -> Fit on train, transform on train and val\nxtrain_svd = svd.transform(xtrain_tfv)                  #Whereas TF-IDF or CountVectorizer -> Fit on BOTH together\nxvalid_svd = svd.transform(xvalid_tfv)\n\n# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\nscl = preprocessing.StandardScaler()\nscl.fit(xtrain_svd)\nxtrain_svd_scl = scl.transform(xtrain_svd)\nxvalid_svd_scl = scl.transform(xvalid_svd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple SVM\nclf = SVC(C=1.0, probability=True) # since we need probabilities\nclf.fit(xtrain_svd_scl, ytrain)\npredictions = clf.predict_proba(xvalid_svd_scl)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we try Grid Search by initializing a pipeline and a parameter grid to iterate through,\n<br> until we find the best set of hyperparameters that yields the *lowest* metric score.\n<br><br> Can also look at Hyperopt for hyperparameter tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing for Grid Search: Scoring Function\nmll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)\n\n# Pipeline Preparation: Initialize model for 1st step in pipeline\nnb_model = MultinomialNB()\n\n# Create the pipeline, inserting steps inside\nclf = pipeline.Pipeline([('nb', nb_model)])\n\n# Initialize Parameter Grid\nparam_grid = {'nb__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Initialize Grid Search Model\nmodel = GridSearchCV(estimator=clf, param_grid=param_grid, scoring=mll_scorer,\n                                 verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we try using pre-trained word vectors with a larger 'map space' compared to employing tf-idf or countvec.\n<br> We need to first convert the text file of vectors to a dict, and then to two matrices that contain the key and weights respectively.\n<br> Now match the words in our dataset to the key values in GloVe, and obtain a mapping of each comment's words.\n<br> After vectorizing, convert to array and pass train data to an XGBoost model for fitting and evaluation.\n<br><br> The GloVe vectors used here are of dimension N=300 (as seen from the filename '..300d'). There are smaller N values available.\n<br> GloVe vector sets are obtained through SVD as well I think? ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Using pre trained word vectors eg GloVe, word2vec, fasttext\n\n# load the GloVe vectors in a dictionary - preprocess text file into dict of word: vector\nembeddings_index = {}\nf = open('glove.840B.300d.txt')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Not sure what sent2vec does or why it needs to normalize for each sentence.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower().decode('utf-8')\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create sentence vectors using the above function for training and validation set\n# tqdm is a loading bar lol\nxtrain_glove = [sent2vec(x) for x in tqdm(train_texts)]\nxvalid_glove = [sent2vec(x) for x in tqdm(valid_texts)]\n\n# convert sentence vectors to array\nxtrain_glove = np.array(xtrain_glove)\nxvalid_glove = np.array(xvalid_glove)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting a simple xgboost on glove features\nclf = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1, silent=False)\n\nclf.fit(xtrain_glove, ytrain)\npredictions = clf.predict_proba(xvalid_glove)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(yvalid, predictions))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Improving The Model**\n<br>Can add other features by fitting/transforming on a separate vectorizer from the comments, then stack the features.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Improving the model by adding additional features like subreddit, author, etc\n\nsubreddits = train_df['subreddit']\ntrain_subreddits, valid_subreddits = train_test_split(subreddits, random_state=17)\n\n# Initialize 2 different vectorizer instances\ntf_idf_texts = TfidfVectorizer(ngram_range=(1, 2), max_features=50000, min_df=2)\ntf_idf_subreddits = TfidfVectorizer(ngram_range=(1, 1))\n\n# Use the 2 to fit/transform their corresponding data\nX_train_texts = tf_idf_texts.fit_transform(train_texts)\nX_valid_texts = tf_idf_texts.transform(valid_texts)\n\nX_train_subreddits = tf_idf_subreddits.fit_transform(train_subreddits)\nX_valid_subreddits = tf_idf_subreddits.transform(valid_subreddits)\n\n# Finally, stack the vectorized data before passing to a model\nfrom scipy.sparse import hstack\nX_train = hstack([X_train_texts, X_train_subreddits])\nX_valid = hstack([X_valid_texts, X_valid_subreddits])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we try out eli5 to display the ranking of weights in our trained model.\n<br> It will show at the top the most important word used in classification.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# tfi-df vectorize preprocessing step\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n\n# initialize and fit/train a model\nmodel = linear_model.LogisticRegression(C=5., solver='sag')\nmodel.fit(train_X, train_y)\n\n# the important words used for classifying, top features by weight rank\nimport eli5\neli5.show_weights(model, vec=tfidf_vec, top=10, feature_filter=lambda x: x != '<BIAS>')\n\n# or\neli5.show_weights(estimator=tfidf_logit_pipeline.named_steps['logit'],\n                  vec=tfidf_logit_pipeline.named_steps['tf_idf'])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}