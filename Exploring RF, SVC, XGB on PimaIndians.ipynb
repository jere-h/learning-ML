{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sys\nimport matplotlib\nimport seaborn as sns\nimport scipy as sp\nimport IPython\nfrom IPython import display\nimport sklearn \nimport random\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_raw = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n\n# make a copy\ndata1 = data_raw.copy(deep = True)\n\nprint(data_raw.info()) # look at dataset\n# data_raw.sample(10)\ndata_raw.describe() # dataset distribution metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Explore correlation to select, combine features\ncorrelation = data_raw.corr()\nsns.heatmap(correlation, annot=True, cbar=True, cmap=\"RdYlGn\")\n\n# Correlated with OUTCOME: Glucose, BMI, Age, Pregnancies, DPF, Insulin\n# BP correlated with BMI, SkinThick with Insulin -> Combine features\n# Make baseline model first with basic features\n# Maybe discretize glucose and insulin levels?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check distributions of columns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(data1.shape[1], 1, figsize=(10,30), dpi=90)\n\nfor ax, feature in zip(axes.flat, data1.columns):\n    sns.distplot(data1[feature], color=\"skyblue\", ax=ax)\n# see that Pregnancies, Insulin, DiabetesPedigreeFunction, Age are skewed\n# May need to scale depending on model used ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data1.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#replace zero values in Glucose, BP, SkinThickness, Insulin\ndflist = []\nfor i in [0, 1]:\n    df1 = data1.loc[data1['Outcome'] == i]\n    df1 = df1.replace({'BloodPressure':0}, np.median(df1['BloodPressure']))\n    df1 = df1.replace({'Glucose':0}, np.median(df1['Glucose']))\n    df1 = df1.replace({'SkinThickness':0}, np.median(df1['SkinThickness']))\n    df1 = df1.replace({'Insulin':0}, np.median(df1['Insulin']))\n    dflist.append(df1)\ndata1 = pd.concat(dflist)\n    \n#default is 0.75 to 0.25 split\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1.drop(columns='Outcome'), \n                                                                        data1['Outcome'], \n                                                                        random_state = 0,\n                                                                       stratify = data1['Outcome'])\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\ntrain1_x.head()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x in data1:\n    if data1[x].dtype != 'float64' :\n        print('Outcome Correlation by:', x)\n        print(data1[[x, 'Outcome']].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data for BASELINE model using given features\n\nfeat_cols = ['Glucose', 'Age', 'BMI', 'Pregnancies', 'DiabetesPedigreeFunction',\n            'Insulin', 'BloodPressure', 'SkinThickness']\n\n# 89% training 83% baseline test acc, without Skin Thickness\n# WITh skin thickness: 90, 85","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare data \n# Correlated with OUTCOME: Glucose, BMI, Age, Pregnancies, DPF, Insulin\n# BP correlated with BMI, SkinThick with Insulin -> Combine features\n# Make baseline model first with basic features\n# Maybe discretize glucose and insulin levels?\n\ndata_cleaner = [train1_x, test1_x]\nfor df in data_cleaner: \n    # create new features lol\n    df['BPxBMI'] = df['BloodPressure']*df['BMI']\n    df['STxINSUL'] = df['SkinThickness']*df['Insulin']\n    # Bin into discrete groups\n    df['GluBin'] = pd.cut(df['Glucose'].astype(int), 5)\n    df['InsBin'] = pd.cut(df['Insulin'].astype(int), 5)\n    df['AgeBin'] = pd.cut(df['Age'].astype(int), 5) \n    #convert to category with LabelEncoder\n    df['GluBin_Code'] = LabelEncoder().fit_transform(df['GluBin'])\n    df['InsBin_Code'] = LabelEncoder().fit_transform(df['InsBin'])\n    df['AgeBin_Code'] = LabelEncoder().fit_transform(df['AgeBin'])\n    \n    \nfeat_cols = ['GluBin_Code', 'InsBin_Code', 'AgeBin_Code', 'Pregnancies', 'DiabetesPedigreeFunction',\n             'BPxBMI', 'STxINSUL']\n\n# 88% training, 85% test accuracy with new features, without BP,BMI,SkinThick\n# 88%, 83% WITH BP, BMI, SkinThick, and new features - probably double represented","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#try another set of features, discretize glucose and insulin\n\ndata_cleaner = [train1_x, test1_x]\nfor df in data_cleaner:  # create features\n    df['GluBin'] = pd.cut(df['Glucose'].astype(int), 5)\n    df['InsBin'] = pd.cut(df['Insulin'].astype(int), 5)\n    df['AgeBin'] = pd.cut(df['Age'].astype(int), 5) \n    #convert to category with LabelEncoder\n    df['GluBin_Code'] = LabelEncoder().fit_transform(df['GluBin'])\n    df['InsBin_Code'] = LabelEncoder().fit_transform(df['InsBin'])\n    df['AgeBin_Code'] = LabelEncoder().fit_transform(df['AgeBin'])\n    \nfeat_cols = ['GluBin_Code', 'InsBin_Code', 'AgeBin_Code', 'Pregnancies', 'DiabetesPedigreeFunction']\n\n#Without including BP, BMI, SkinThick 70, 75%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set to RF classifier model, n_est tuned at 15, ensemble bagging forest type\nmodel = RandomForestClassifier(n_estimators = 200,\n                              max_features = 'sqrt')\nmodel.fit(train1_x[feat_cols], train1_y)\n\n#Setting best model as one with new features, adjusting hyperparams\n#Results at 88, 85%, tuning min_samples_leaf 2-5 no change","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set to SVC large margin classifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n#Affected by distance, need to scale first\nscaler = StandardScaler().fit(train1_x[feat_cols])\nmodel = SVC(kernel = 'poly', gamma=1, degree = 3)\nmodel.fit(scaler.transform(train1_x[feat_cols]), train1_y)\n\n#doesnt do very well... might need to tune","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Set to XGBoost ensemble gradient descent boosting\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n#Affected by distance, need to scale first only on train set, fit to both train and test/val set\nscaler = StandardScaler().fit(train1_x[feat_cols])\n\nmodel = XGBClassifier(n_estimators = 100, learning_rate = 0.01)\nmodel.fit(scaler.transform(train1_x[feat_cols]), train1_y, verbose = False)\n\n#XGBoost does ok, around 88, 83. Random Forest still seems better\n#Tuning n_est and learning rate","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict, Evaluate using xval accuracy \n# No scaling here, forest not affected \nfrom sklearn import ensemble   # set up model\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\n\ncv = ShuffleSplit(n_splits = 100, test_size = 0.25, random_state = 0)\n\ntrainAcc = round(np.median(cross_val_score(model, train1_x[feat_cols], train1_y, cv = cv)),2)*100\ntestAcc = round(np.median(cross_val_score(model, test1_x[feat_cols], test1_y, cv = cv)),2)*100\n\n# f1score = f1_score(train1_y, y_pred) * 100\n# Accuracy = round(np.median(cross_val_score(model, test1_x[feat_cols], test1_y, cv = cv)),2)*100\n\nprint('Acc on train set: ', trainAcc, '%')\nprint('Acc on test set: ', testAcc, '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaled Version for distance affected models like SVM, XGBoost\n# Predict, Evaluate using xval accuracy \nfrom sklearn import ensemble   # set up model\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\n\ncv = ShuffleSplit(n_splits = 100, test_size = 0.25, random_state = 0)\n\ntrainAcc = round(np.median(cross_val_score(model, scaler.transform(train1_x[feat_cols]), train1_y, cv = cv)),2)*100\ntestAcc = round(np.median(cross_val_score(model, scaler.transform(test1_x[feat_cols]), test1_y, cv = cv)),2)*100\n\n# f1score = f1_score(train1_y, y_pred) * 100\n# Accuracy = round(np.median(cross_val_score(model, test1_x[feat_cols], test1_y, cv = cv)),2)*100\n\nprint('Acc on train set: ', trainAcc, '%')\nprint('Acc on test set: ', testAcc, '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}